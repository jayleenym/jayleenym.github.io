---
layout: post
title: 차원축소 Dimension Reduction
subtitle: KAIST AI대학원 주재걸 교수님
categories: ML
tags: [ML, 차원축소, PCA]
usemathjax: true
---

$$Feature = Attribute = Variable = Dimension$$
# Benefits of Dimension Reduction
- 적은 용량
- 빠른 계산
- **노이즈 제거**를 통한 data quality 향상
- **2D/3D 시각화 가능**
![image](/assets/images/posts/220519-1.png)


# 1. Feature Selection
- 차원 축소를 위해 여러 feature 중 task에 도움이 될 feature를 골라내는 작업
- 특정한 **목적**을 가지고, 전처리하는 과정으로 이루어짐

### Forward Selection
![image](/assets/images/posts/220519-2.png)

- 총 100개 feature 후보가 있다고 하면, 1개씩 feature로 학습
- 변수가 늘어날수록 학습 데이터의 성능은 점점 좋아짐 (loss는 점점 떨어짐)
- 하지만 valid data(unseen data)에 대해 성능이 어느 순간부터 떨어짐 = 이 때 가장 성능이 좋았던 4개 feature 조합을 찾아내는 것
- 어떤 것을 가장 먼저 추가할 것인가? 각각 1d으로 regression을 돌림. 총 100가지 서로 다른 feature를 사용했을 때 만들어지는 모델의 성능을 알 수 있음
- validation data의 성능을 가지고, 가장 좋은 성능을 가지는 feature를 먼저 선정.

### Backward Elimination
![image](/assets/images/posts/220519-3.png)

- foward와 반대방향
- 100개에서 하나씩 제외하며 validation 성능을 보는 것

### Correlation Coefficient
![image](/assets/images/posts/220519-4.png)

- 좀 더 명확한 패턴을 가지는 variable를 찾는 것이 target에 더 좋은 feature다!
- 상관성이 있느냐, 없느냐를 측정해서 feature 선정
- validation 성능을 보지 않고도 selection을 진행할 수 있는 방법


## 2. Feature Extraction
- feature가 다 참여하는데, 몇몇 feature가 적절한 가중치를 가지고 low dimensional vector에 참여하는 느낌
- linear나 logistic regression에서, 

### PCA (Principal Component Analysis)
- data를 scatter plot으로 표현했을 때, 고차원 공간 상에서 차원 축소시 **projection**을 통해 데이터들을 mapping해나가는 것   
*Projection? 특정한 방향을 정해서 선에 수선의 발 위치에 점을 찍고, 원점으로부터 그 점들을 이어주는 것
- low dimension 수만큼 선 선택

*Q. 선을 어떻게 잡는 것이 좋은가?*  
*A. **데이터가 최대한 많은 변화 폭을 갖는 선으로 정하자!***
- 분산 값이 큰 쪽으로 계산을 하게 됨  
(1번이 2번보다 좋다)
![image](/assets/images/posts/220519-5.png)
- 왜? original space에서의 거리를 가장 잘 반영할 수 있는 것이기 때문에!

#### word2vec
사전에 지정해 놓은 단어들의 vector DB ≒ GloVe
- 단어 하나하나를, word = data item으로 보고 이들의 feature space 내에 있는 하나의 점들로 볼 수 있음
- 만약 단어가 10d라고 한다면,
    - word를 vector로 나타낼 수 있음
    - 문서? 10d짜리 vector들을 (순서를 고려해) 잘 조합한 것
- 단어 간 유사도를 고려해서 

### Multi-dimensional PCA

### Linear Dimension Reduction


### Multi-Dimensional Scaling (MDS)
![image](/assets/images/posts/220519-6.png)
![image](/assets/images/posts/220519-7.png)

PCA와 비슷. 본래 거리 정도를 최대한 유지하자!

- PCA: 축을 통해 좌표값을 얻어냈음
- MDS: 각각의 low-dim distance를 편미분해서 최소값을 찾음. 각 좌표를 독립적으로 gradient decent해서 미세 조정.

<img src = "/assets/images/posts/220519-8.png" width="60%" align="center">

ex1. 8이어야하는데 계산해 보니 5더라 → 더 멀어지는 방향으로 x1, x2, x3를 update.   
ex2. 1이어야 하는데 계산해보니 2더라 → 더 가까워지는 방향으로 점의 좌표값을 조정
- 최적화하는 대상? **100단어 각각의 N차원 vector 업데이트**


### t-SNE [끝판왕]
오래걸림. 모든 pari-wise distance를 **계산함** => Ground-truth distance 정보를 만들고 시작.
![image](/assets/images/posts/220519-9.png)
- DH와 DL의 차이가 최소화가 될 수 있도록 계속 업데이트 (iterative update)
- 집단 간 공간적인 여유도 줌. 명확히 구분되는 패턴이 보임
- PCA는 identity ~ 로 계산하면 바로 나옴

**왜 t-SNE를 써야 하는가**  
시각화!!!를 통해 제대로 볼 수 있다!
PCA는 reduction 계산용으로 많이 쓰임.